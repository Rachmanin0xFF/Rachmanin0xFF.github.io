<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=format-detection content="telephone=no"><meta name=HandheldFriendly content=true><meta name=MobileOptimized content=320><meta name=viewport content="initial-scale=1,width=device-width"><title>Adam Lastowka - Order, Multisets, and Language</title><meta name=description content=""><script async defer src=./../../scripts/parallax.js onload="var loaded=true;"></script><link rel=stylesheet id=lightmodehl href=./../../scripts/highlight/styles/atom-one-light.min.css><link rel=stylesheet id=darkmodehl href=./../../scripts/highlight/styles/hybrid.min.css disabled><script src=./../../scripts/highlight/highlight.min.js></script><script>hljs.highlightAll();</script><link rel=stylesheet href=./../../post.css><script defer src=./../../scripts/darkmode.js></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=icon href=./../../favicon.ico type=image/x-icon><link rel="shortcut icon" href=./../../favicon.ico type=image/x-icon></head><body><div id=bkg><section id=not-background><section id=sidebar><div class=sticky><div class="sb-big onbkg hvr-rotate"><a href=../../index.html>Home</a></div><br><div class="sb-big onbkg hvr-rotate"><a href=../../about/index.html>About</a></div><br><div class="sb-big onbkg hvr-rotate"><a href=../../qa/index.html>Q&A</a></div><br><br><div id=dm-toggle class="button sb-big hvr-rotate">Lights</div></div></section><section id=header><div class=null><div id=spikes class=vector alt=""></div></div><div id=sitename><h5><a href=./../../index.html>Adam<br>Lastowka</a></h5></div></section><div id=foreground><section id=feed><h1>Order, Multisets, and Language</h1><div class=date>Published: Aug 2, 2024</div><div class=tags>Tagged with: <a href=./../../topics/cs/index.html>cs</a> <a href=./../../topics/information/index.html>information</a> <a href=./../../topics/math/index.html>math</a></div><br><h2 id=multisets--order>Multisets &amp; Order</h2><p>Order matters a whole lot.</p><p>A gigabyte of memory is useful because it has order. If we remove that order, we&#39;re left with a jumbled bucket of \(8\times 10^9\) indistinguishable bits, and the only information we have left is &quot;how many ones are in the bucket&quot;. Suddenly, the information collapses from \(8\times 10^9\) bits into just \(\log_2(8\times 10^9)\approx 33\) bits.</p><p>This bag-like data structure (one with unordered, possibly repeating elements) is called a <strong>multiset</strong>. In the &#39;bucket-of-bits&#39; example above, the multiset was formed by pulling eight billion symbols from the set \(\{0, 1\}\). Instead, if we partition our gigabyte into <em>bytes</em> (instead of bits), we pull a billion symbols from the 256-member set \(\{0, 1, ..., 255\}\). This is a more realistic use case for multisets: maybe we&#39;re storing ASCII characters. To generalize, we&#39;ll speak of a multiset with \(k\) members, each of which belongs to one of \(n\) indistinguishable types. So for 1GB partitioned into 8-bit segments, \(n=256\) and \(k=10^9\).</p><p>The number of unique multisets for some \(n, k\) is written \( \left(\!\!{n\choose k}\!\!\right) \) and is said \(n\) <strong>multichoose</strong> \(k\). It has a nice expression in terms of the binomial coefficient:</p><p>\[ \left(\!\!{n\choose k}\!\!\right) = {n + k - 1 \choose k} = \frac{n(n+1)(n+1)\cdots(n+k-1)}{k!} \]</p><p>From this, we can easily find the (asymptotic for large \(n,k\)) information content of a multiset using <a href=https://en.wikipedia.org/wiki/Stirling%27s_approximation>Stirling&#39;s approximation</a> (\(\ln(n!)=n\ln(n)-n+O(\ln n)\)):</p><p>\[ \log_2\left(\!\!{n\choose k}\!\!\right)\approx\frac{1}{\ln(2)}\big[(n+k-1)\ln(n+k-1)-k\ln(k)-(n-1)\ln(n-1)\big] \]</p><p>\[ \approx (k+n)\log_2(k+n) - (k\log_2(k) + n\log_2(n)),\ n,k \gg 1 \]</p><p>This is a solid approximation: for our gigabyte example (\(n=256,k=10^9\)), it gives us ~5975.2 bits, just 0.5% over the actual value of 5947.8 bits. If we divide this expression by the ordered information, \(n*\log_2(k)\), we can subtract that fraction from 100% and answer an interesting question: what percentage of our information is encoded in the order of our symbols? The answer, in general, is given below (recall that Stirling&#39;s approximation assumes \(n,k \gg 1\)):</p><p>\[ \%I_\textrm{order}=100\% - \frac{1}{k \ln(n)}\ln\left(\!\!{n\choose k}\!\!\right) \approx 100\% - \frac{(k+n)\ln(k+n)-(k\ln(k)+n\ln(n))}{k \ln(n)} \]</p><p>The results of this are reasonable: in 1GB of bytes, 99.99993% of the bytes&#39; information is encoded in the order of the 8-bit segments. Meanwhile, in a collection of a thousand 32-bit integers, that figure drops down to 27%, with 73% of the information stored in the choice of integers. Finally, for an ordered pair of 32-bit integers, their order accounts for just 1.6% of the total information contained in the pair.</p><h2 id=language>Language</h2><p>There&#39;s no reason why we can&#39;t apply the formula above to the English language (or any other language). Although the actual information density of English words varies wildly, <a href=https://en.wikipedia.org/wiki/Claude_Shannon>Shannon</a> put it at approximately <a href=https://archive.org/details/bstj30-1-50>12 bits per word</a>. The average sentence length in this article so far is 16 words, so we&#39;ll say a &#39;typical sentence&#39; consists of 16 words, each of which is represented in some perfect 12-bit encoding.</p><p>Plugging in \(n=2^{12},k=16\), we find that only <strong>roughly a quarter of the information in a typical English sentence is encoded in the order of its words</strong>. This is a dramatic result that, intuitively, feels incorrect. Long to are not always sentences scrambled interpret, especially when they are easy [<em>scrambled sentences are not always easy to interpret, especially when they are long</em>].</p><p>But we are biased: our sequential word-processing architecture isn&#39;t designed to extract meaning from jumbled text. Meanwhile, computers don&#39;t care nearly as much about word order. Natural language understanding algorithms are <a href=https://arxiv.org/abs/2012.15180>surprisingly invariant</a> under in-sentence word shuffling.</p><p>In sum, order matters... except when it comes to text. Keep this in mind when writing avant-garde poetry, I guess.</p><h3 id=food-for-thought>Food For Thought:</h3><ul><li>How close can multiset implementations get to their theoretical maximum space-efficiency?</li><li>How much does order matter in images (investigate graining &amp; shuffling)?</li><li>How can we empirically measure the percentage of sentence information encoded in order?</li></ul><a href=./../../articles\RandomWalkSDF\index.html class=button2>Previous Post:<br>PDEs - Random Walks and Distance Fields</a></section><div id=footer></div></div></section></div></body></html>