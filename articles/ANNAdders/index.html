<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=format-detection content="telephone=no"><meta name=HandheldFriendly content=true><meta name=MobileOptimized content=320><meta name=viewport content="initial-scale=1,width=device-width"><title>Adam Lastowka - Binary Universal Approximation &amp; Neural Logic</title><meta name=description content=""><script async defer src=./../../scripts/parallax.js onload="var loaded=true;"></script><link rel=stylesheet id=lightmodehl href=./../../scripts/highlight/styles/atom-one-light.min.css><link rel=stylesheet id=darkmodehl href=./../../scripts/highlight/styles/hybrid.min.css disabled><script src=./../../scripts/highlight/highlight.min.js></script><script>hljs.highlightAll();</script><link rel=stylesheet href=./../../post.css><script defer src=./../../scripts/darkmode.js></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script type=text/x-mathjax-config>MathJax = {
    tex: {
      inlineMath: [['$', '$'], ["\\(", "\\)"]],
      processEscapes: true,
    }
  }</script><link rel=icon href=./../../favicon.ico type=image/x-icon><link rel="shortcut icon" href=./../../favicon.ico type=image/x-icon></head><body><div id=bkg><section id=not-background><section id=sidebar><div class=sticky><div class="sb-big onbkg hvr-rotate"><a href=../../index.html>Home</a></div><br><div class="sb-big onbkg hvr-rotate"><a href=../../about/index.html>About</a></div><br><div class="sb-big onbkg hvr-rotate"><a href=../../qa/index.html>Q&A</a></div><!--<br><br><div id="dm-toggle" class="button sb-big hvr-rotate">Lights</button>--></div></section><section id=header><div class=null><div id=spikes class=vector alt=""></div></div><div id=sitename><h5><a href=./../../index.html>Adam<br>Lastowka</a></h5></div></section><div id=foreground><section id=feed><h1>Binary Universal Approximation &amp; Neural Logic</h1><div class=date>Published: Sep 8, 2024</div><div class=tags>Tagged with: <a href=./../../topics/math/index.html>math</a> <a href=./../../topics/ml/index.html>ml</a> <a href=./../../topics/cs/index.html>cs</a></div><br><h3 id=the-depth-necessary-for-addition>The Depth Necessary for Addition</h3><p>In 2023, <a href=https://cprimozic.net/ >Casey Primozic</a> wrote <a href=https://cprimozic.net/blog/reverse-engineering-a-small-neural-network/ >an awesome blog post</a> about training a small neural network to perform modular 8-bit addition. The network&#39;s solution was interesting — you can read about it in the post; it&#39;s not unlike what a similar network <a href=https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Modular_Addition>reverse-engineered by Neil Nanda and Tom Lieberum in 2022</a> achieved. Here, I want to focus on a hypothesis Casey stated at the beginning of his post:</p><blockquote><blockquote><p><em>Additionally, I wasn&#39;t sure how the network would handle long chains of carries. When adding 11111111 + 00000001, for example, it wraps and produces an output of 00000000. In order for that to happen, the carry from the least-significant bit needs to propagate all the way through the adder to the most-significant bit. I thought that there was a good chance the network would need at least 8 layers in order to facilitate this kind of behavior.</em></p></blockquote></blockquote><p>Obviously, Casey then showed that this was false, but it&#39;s a very reasonable way of thinking. Look at a circuit diagram of <a href=https://en.wikipedia.org/wiki/Carry-lookahead_adder#Implementation_details>any adder</a>, and you&#39;ll probably see a sequential chain of gates flowing from lower to higher bit significance. For \(n\) bits, <a href=https://en.wikipedia.org/wiki/Kogge%E2%80%93Stone_adder>the best adder circuits</a> still need \(\log_2(n)\) layers. Additionally, the DAC/ADC-style solution Casey&#39;s network found might not scale well, and it would likely hit the floating-point precision floor as its bit count increased (this might be interesting to test). Maybe adders <em>do</em> need depth.</p><p>But wait, what about <a href=https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary_width>universal approximation</a>?</p><p>Actually, circuit theory also has something to say here. Neural networks have what&#39;s called &quot;unlimited <a href=https://en.wikipedia.org/wiki/Fan-in>fan-in</a>&quot; — a neuron can have as many inputs as it likes. Couple this with a nonlinear activation function, and you&#39;ll find that, with the right parameters, individual neurons can perform AND and OR operations on an arbitrary number of incoming &#39;bits&#39;, as we&#39;ll see soon.</p><p>Because neurons have unlimited fan-in, feed-forward neural nets should be able to do addition with a constant number of layers, regardless of how many bits they&#39;re operating on: addition belongs to the \(AC^0\) circuit complexity class, which has \(O(1)\) depth and polynomial size. Let&#39;s see how we can construct such a network explicitly.</p><h3 id=neuron-logic>Neuron Logic</h3><p>First, we need to recognize that conventional neural networks can act as binary operators. A single ReLU-activated neuron can act as an AND gate:</p><div class=imblock><img src=reluand.png class=postim></div><p>While it takes two ReLU neurons to make an OR:</p><div class=imblock><img src=reluor.png class=postim>With a sigmoid activation, you can use a single neuron and saturate it by cranking up the weights.</div><p>Making a NOT gate is also easy (weight -1, bias 1). We could call it here and cite some circuit theory textbook, but let&#39;s explore a naive way to actually develop a \(O(1)\) (i.e. constant width) \(n\)-bit adder.</p><h3 id=coding-an-adder>&quot;Coding&quot; an Adder</h3><p>Let&#39;s consider a 2-bit adder. We can explicitly write out the truth table for one of these:</p><div class=imblock><img src=2bittruth.png class=postim></div><p>There are 16 entries in this truth table, representing each pair of 2-bit numbers that could be added together. The truth table for a 16-bit adder would have \(2^{32}\), or 4 billion entries.</p><p>The easiest way to translate this into a constant-depth circuit? Encode each of these four billion rules into its logic. This means that our initial 16-bit circuit will have billions of transistors, but that&#39;s fine for now, we&#39;re just exploring.</p><p>First, we consider each digit of the output separately. Then, for each rule in the truth table with an output of &#39;1&#39;, we make a circuit that activates <em>if and only if</em> each precondition in the table is met. Finally, we join the outputs of all these circuits with an unlimited fan-in OR gate. For our purposes, this is equivalent to writing the logic for each output digit in its Disjunctive Normal Form (DNF).</p><p>For example, for input binary numbers \((a_2, a_1)\) and \((b_2, b_1)\), the 10s place bit is given by: \[ (a_1 \land b_1) \oplus (a_2 \oplus b_2) \] Which, in DNF, expands to: \[ (\lnot a_2 \land \lnot a_1 \land b_2) \lor(\lnot a_2 \land b_2 \land \lnot b_1) \lor \cdots \lor (a_2 \land a_1 \land b_2 \land b_1) \] As a circuit, that expression looks like this:</p><div class=imblock><img src=10splacecircuit.png class=postim>If you're doing this with neurons, you can make things a little more compact by folding the NOTs into the AND gate's weights.</div><p>This technique generalizes to <em>any</em> arbitrary truth table. However, it&#39;s a pretty disgusting way to make a circuit. Addition is a low-entropy operation; it has exploitable symmetries that suggest more efficient constant-depth representations. For \(n\) bits, this one has a width (AND gate count) in \(O(4^n)\). You can get that down to a 3-layer \(O(n^3)\) if you&#39;re <a href=https://www.csa.iisc.ac.in/~chandan/courses/arithmetic_circuits/notes/lec4.pdf>clever about it</a> (remember, addition is in \(AC^0\), which means polynomial size). That&#39;s not too bad! For a 16-bit adder, \(O(n^3)\) puts us on the order of \(10^4\) gates, which is reasonable for a neural network.</p><p>Interestingly, the fact that we can construct this kind of a network at all <strong>demonstrates a binary universal approximation theorem</strong>. All data on computers is stored in binary, and this shows that a shallow neural network can transform that data in any way you want.</p><h3 id=in-the-wild>In the Wild</h3><p>But, like all the other universal approximation theorems, it&#39;s not very helpful when you&#39;re trying to train something. If a network memorizes its inputs without &#39;understanding&#39; them, we call it &quot;overfitting&quot;. Sometimes this is okay: networks can still <a href=https://www.beren.io/2022-01-11-Grokking-Grokking/ >grok</a> data after overfitting it, especially in sufficiently-overparameterized networks. Maybe that initial gradient rush to the solution manifold sometimes takes us to a network similar to the DNF nets described above? Maybe DNF is a good starting point?</p><p>No, and probably not.</p><p>First, neural nets rely heavily on dense layers, where neurons are fully connected to each other. The type of network we&#39;ve constructed is extremely sparse, and gradient descent is unlikely to accidentally stumble across anything like it unless it&#39;s being heavily regularized. I&#39;ve been trying to coax a network into this sort of representation for a while, but without fixing the topology via weight masking, it&#39;s incredibly challenging.</p><p>Second, DNF can&#39;t really <em>be</em> abstract; it lays everything out on the table, and doesn&#39;t permit any clever representation tricks. Densely-connecting a DNF network <em>might</em> be a good starting point for understanding grokking, though (or slowly &#39;activating&#39; additional weights), but probably not on anything of significant size.</p><h3 id=takeaways>Takeaways</h3><ul><li>Any boolean function can be represented as a constant-depth neural network by writing each output bit as an expression of the input bits in disjunctive normal form, then creating a logical (neural) circuit from that expression.</li><li>However, these networks are sparsely-connected, and non-polynomial in width, and probably incapable of generalizing.</li></ul><a href=./../../articles\SymbolicCircuits\index.html class=button2 style=min-width:47%;>Previous Post:<br>Optimizing Simplification in Symbolic Circuit Analysis </a><a href=./../../articles\UnorderedList\index.html class=button2 style=min-width:47%;>Next Post:<br>Order, Multisets, and Language</a></section><div id=footer></div></div></section></div></body></html>