<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=format-detection content="telephone=no"><meta name=HandheldFriendly content=true><meta name=MobileOptimized content=320><meta name=viewport content="initial-scale=1,width=device-width"><title>Adam Lastowka - Resampling I - The Lost Art of Error Diffusion</title><meta name=description content=""><script async defer src=./../../scripts/parallax.js onload="var loaded=true;"></script><link rel=stylesheet id=lightmodehl href=./../../scripts/highlight/styles/atom-one-light.min.css><link rel=stylesheet id=darkmodehl href=./../../scripts/highlight/styles/hybrid.min.css disabled><script src=./../../scripts/highlight/highlight.min.js></script><script>hljs.highlightAll();</script><link rel=stylesheet href=./../../post.css><script defer src=./../../scripts/darkmode.js></script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><link rel=icon href=./../../favicon.ico type=image/x-icon><link rel="shortcut icon" href=./../../favicon.ico type=image/x-icon></head><body><div id=bkg><section id=not-background><section id=sidebar><div class=sticky><div class="sb-big onbkg hvr-rotate"><a href=../../index.html>Home</a></div><br><div class="sb-big onbkg hvr-rotate"><a href=../../about/index.html>About</a></div><br><div class="sb-big onbkg hvr-rotate"><a href=../../qa/index.html>Q&A</a></div><br><br><div id=dm-toggle class="button sb-big hvr-rotate">Lights</div></div></section><section id=header><div class=null><div id=spikes class=vector alt=""></div></div><div id=sitename><h5><a href=./../../index.html>Adam<br>Lastowka</a></h5></div></section><div id=foreground><section id=feed><h1>Resampling I - The Lost Art of Error Diffusion</h1><div class=date>Published: Jul 31, 2023</div><div class=tags>Tagged with: <a href=./../../topics/math/index.html>math</a> <a href=./../../topics/graphics/index.html>graphics</a> <a href=./../../topics/images/index.html>images</a> <a href=./../../topics/cs/index.html>cs</a></div><br><div class=imblock><img src=input.png class=postim></div><p>This is a digital photo of a synthetic clothesline against a mostly-clear sky. Each pixel inhabits an 8-bit monochromatic color space; i.e., each pixel is represented by a <em>byte</em>, some integer \(\in [0, 255]\). In most display environments, this is enough precision to fool our eyes into believing that the colors in the image are continuous: I can&#39;t easily see any sort of quantization (not spatially nor in terms of value) when I display the image on my monitor.</p><p>For irrelevant reasons, say I need to display this image on another monitor. Unfortunately, this new monitor&#39;s pixels only have two states: on and off. Naively, I say, &quot;Alright, I&#39;ll only turn on a pixel if it&#39;s brightness is greater than 100.&quot;</p><div class=imblock><img src=input_naive.png class=postim></div><p>Oh, no. The original photo was bad, but this is allmost indiscernible. The wood&#39;s texture has been completely lost, and from out of nowhere, a noisy line splits the sky into light and dark. Distraught, I try something drastic: I add some random noise to the photo.</p><div class=imblock><img src=input_noisy.png class=postim></div><p>Now, I apply the exact same cutoff at a brightness of 100:</p><div class=imblock><img src=input_random.png class=postim></div><p>Suddenly, the noisy line vanishes, we can begin to see the texture of the wood, and our one-bit monitor has a photo that somewhat resembles the original. What happened here? Why did this work? And can we do better?</p><h1 id=dither>Dither</h1><p>Paradoxically, <em>adding</em> noise to a signal can often increase its fidelity after quantization (or decimation, bit reduction, compression — whatever you want to call it). Noise added for this purpose is called <strong>dither</strong>.</p><div class=imblock><img src=dither_signals.png class=postim>The noise in the last waveform is called dither.</div><p>Dither is commonly used in digital audio, digital image processing, and many other fields (radar, seismology, etc.). But not all dither is created equal. The version I used above — white noise dithering — is one of the worst. Curiously, the &quot;goodness&quot; of a dither is directly tied to its frequency spectrum, which we can obtain from an image via a 2D Fourier transform.</p><h1 id=noise-spectra>Noise Spectra</h1><p>Let&#39;s look at the <a href=https://commons.wikimedia.org/wiki/File:2D_Fourier_Transform_and_Base_Images.png>frequency-space representations</a> (sometimes called a periodogram) of our clothesline photo and its two quantized versions:</p><div class=imblock><img src=structure_preservation.png class=postim>Pixels closer to the center of the periodogram correspond to lower frequencies; brighter pixels indicate higher amplitudes of these frequencies in the source image. The bright lines running through the periodogram represent long, sharp edges in the source photo. The periodograms in this article only display log-scaled complex modulous; phase information is discarded.</div><p>Atr a glance, the naively-quantized spectrum (center) more closely resembles the source image spectrum than its noisy counterpart: the &quot;diffraction spikes&quot; extend much further, and the picture appears clearer. But <em>near the center</em> of the periodogram (right), we see that our naive quantization has added an extra horizontal spike to our periodogram and altered the intensities of its diagonal spikes. Meanwhile, the noisy quantization&#39;s spectrum looks like the source image&#39;s near its center, but it retains none of the high frequencies.</p><p>The first takeaway is that noise specifically reduces <em>low-frequency</em> quantization error. The second is that our eyes <em>don&#39;t care</em> about high-frequency information. Sure, we appreciate sharp images, but the bulk of &quot;useful&quot; information in most photos is in its broad, low-frequency structure. A <em>gedankenexperiment</em> to illustrate my point: does downscaling a photo by a facotr 2 in each direction reduce its human-appreciable information content by a factor 4? For most of the images we see on our screens, absolutely not! In many ways, this is unsurprising: when approximating a function via Fourier series, the 1st and 2nd order decrease the error far more than the 101st and 102nd. This is because most functions (and sights) we encounter in the physical world are restrained by continuity, making high-amplitude, high-frequency noise rare (hooray for entropy).</p><p>Sorry, that was a little abstract. I&#39;ll summarize my point with a diagram:</p><div class=imblock><img src=importance.png class=postim></div><p>Actually, the area outside the center of the periodogram is so unimportant to our eyes that we can just fill it with noise:</p><div class=imblock><img src=lowpass_spectrum.png class=postim></div><p>If we try to reconstruct the image from this data, we get a low-passed version of our photo:</p><div class=imblock><img src=lowpass.png class=postim></div><p>Although we removed ~90% of its frequency-space information, the photo still looks pretty much like it used to.</p><h1 id=blue-noise>Blue Noise</h1><p>We know that noise (dither) helps preserve low-frequency detail after quantization, but at the same time, white noise has a flat spectrum, and thus a low-frequency component that muddles an image when it&#39;s applied. What if, instead, there were another type of noise we could add <em>without</em> any low-frequency component? Enter <strong>blue noise</strong>:</p></section><div id=footer></div></div></section></div></body></html>