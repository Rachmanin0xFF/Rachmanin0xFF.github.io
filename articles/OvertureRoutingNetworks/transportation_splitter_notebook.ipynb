{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```MIT License\n",
    "\n",
    "Copyright (c) 2024 Adam Lastowka\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Local Transportation Networks With Overture Maps Data\n",
    "\n",
    "[Overture Maps](https://overturemaps.org/) releases [transportation data](https://docs.overturemaps.org/guides/transportation/) with a unique, comprehensive schema. Unfortunately, it's a little tricky to route with.\n",
    "\n",
    "Overture's data consists of *segments*: LineStrings representing paths (roads, walkways, train tracks, etc.). These segments contain many linearly-referenced features, including *connectors*: places where you can hop from one path to another (intersections, off-ramps, etc.). There's no practical limit on the number of connectors a segment can have; a highway can have many off-ramps.\n",
    "\n",
    "Unfotunately, this schema isn't exactly conducive to routing. Routing frameworks expect *directed graphs*, where each edge is gaurenteed to be connected to only two nodes. In its native form, Overture's data is *almost* a [hypergraph](https://en.wikipedia.org/wiki/Hypergraph), where 'connectors' are nodes and 'segments' are (hyper-)edges — but edges in hypergraphs are typically unordered sets, so this isn't quite right, either.\n",
    "\n",
    "In any case, unless we want to write a novel routing engine from scratch (which might take a while; contraction hierarchies are messy), we'll need to somehow transform Overture's data into a directed graph. That means splitting apart each segment at each of its connectors. Thankfully, the hard part has already been done: we'll use ibnt1/brad-richardson/seanmcliroy29's [transportation-splitter](https://github.com/OvertureMaps/transportation-splitter). Let's get that running.\n",
    "\n",
    "*Note: This has been [done before](https://www.crunchydata.com/blog/vehicle-routing-with-postgis-and-overture-data), but not in a way that I can easily run on my PC! I'll be reworking this example with a local, open-source stack.*\n",
    "\n",
    "## Running transportation-splitter Locally\n",
    "\n",
    "`transportation_splitter.py` uses Apache Spark — a powerful, distributed analytics engine — to do its heavy lifting. Setting up spark locally is a little awkward, but it'll probably take less time than rewriting all the UDFs and operations to use something like DuckDB ([Ibis](https://ibis-project.org/) might simplify this).\n",
    "\n",
    "To get the script working, I had to (updated Dec. 18, 2024):\n",
    "1. Install [Apache Spark 3.5.2](https://archive.apache.org/dist/spark/spark-3.5.2/) (not 3.5.3, version numbers are very important here)\n",
    "2. Run `pip install pyspark==3.5.2 apache-sedona pyproj shapely`\n",
    "3. Download the Windows [Hadoop Binaries, v3.5.3](https://github.com/cdarlint/winutils) (my Linux box is dead)\n",
    "5. Set the following environment variables:\n",
    "\n",
    "```powershell\n",
    "PYSPARK_PYTHON=python\n",
    "# spark install directory\n",
    "SPARK_HOME=path/to/spark-3.5.2-bin-hadoop3\n",
    "# folder containing 'bin' folder with Hadoop binaries\n",
    "HADOOP_HOME=path/to/hadoop-3.3.6\n",
    "# add everything to the path\n",
    "PATH=%PATH%;%HADOOP_HOME%\\bin;%SPARK_HOME%\\bin\n",
    "```\n",
    "\n",
    "*Note: You may also need to manually resolve some dependencies by adding something to the path / copying a .jar file somewhere; check your error messages carefully.*\n",
    "\n",
    "Next, `transportation_splitter.py` expects a couple variables to exist when it's run: `spark` and `sc`. We can initialize them locally, using Maven to fetch dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "def get_local_context():\n",
    "    config = SedonaContext.builder().master(\"local[*]\").appName(\"test app\").config('spark.jars.packages', \\\n",
    "    'org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.7.0,' \\\n",
    "    'org.datasyslab:geotools-wrapper:1.7.0-28.5,' \\\n",
    "    'org.apache.hadoop:hadoop-common:3.3.4,' \\\n",
    "    'com.google.code.gson:gson:2.9.0,' \\\n",
    "    'org.apache.hadoop:hadoop-aws:3.3.4,' \\\n",
    "    'org.apache.parquet:parquet-hadoop:1.15.0,' \\\n",
    "    'com.amazonaws:aws-java-sdk-bundle:1.12.262') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.us-west-2.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    "    .config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    return SedonaContext.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the key-less `AnonymousAWSCredentialsProvider` because Overture keeps its parquets in a publicly accessible s3 bucket!\n",
    "\n",
    "Next, let's import `transportation_splitter.py` (`spark` isn't defined as a global, so nothing should run), and write/run a shallow wrapper method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output/\n",
      "filter_wkt: POLYGON ((-75.247 40.038, -75.247 39.870, -75.077 39.870, -75.077 40.038, -75.247 40.038))\n",
      "write config: SplitterDataWrangler(input_path='s3a://overturemaps-us-west-2/release/2024-11-13.0/theme=transportation', output_path_prefix='file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered')\n",
      "split config: SplitConfig(split_at_connectors=True, lr_columns_to_include=[], lr_columns_to_exclude=[], point_precision=7, lr_split_point_min_dist_meters=0.01, reuse_existing_intermediate_outputs=True)\n",
      "spatially_filtered_path: file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_1_spatially_filtered\n",
      "joined_path: file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_2_joined\n",
      "raw_split_path: file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_3_raw_split\n",
      "segment_splits_exploded_path: file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_4_segments_splits\n",
      "Calling default check exists for step SplitterStep.spatial_filter at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_1_spatially_filtered\n",
      "Calling default write for step SplitterStep.read_input at s3a://overturemaps-us-west-2/release/2024-11-13.0/theme=transportation\n",
      "input_df.count() = 677718292\n",
      "filter_df()...\n",
      "Calling default write for step SplitterStep.spatial_filter at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_1_spatially_filtered\n",
      "filtered_df.count() = 176465\n",
      "lr_columns_for_splitting: \n",
      "['road_surface', 'level_rules', 'subclass_rules', 'speed_limits', 'routes', 'road_flags', 'names', 'access_restrictions', 'prohibited_transitions', 'width_rules']\n",
      "Calling default check exists for step SplitterStep.joined at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_2_joined\n",
      "join_segments_with_connectors()...\n",
      "Calling default write for step SplitterStep.joined at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_2_joined\n",
      "joined_df.count() = 72948\n",
      "Calling default check exists for step SplitterStep.raw_split at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_3_raw_split\n",
      "split_joined_segments()...\n",
      "Calling default write for step SplitterStep.raw_split at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_3_raw_split\n",
      "split_segments_df.count() = 72948\n",
      "split_segments_df\n",
      "total length stats\n",
      "+-------------------------+------------------------+--------------+------------+\n",
      "|total_length_before_split|total_length_after_split|length_removed|length_added|\n",
      "+-------------------------+------------------------+--------------+------------+\n",
      "|                  8345046|                 8314097|        -30948|           0|\n",
      "+-------------------------+------------------------+--------------+------------+\n",
      "\n",
      "Calling default check exists for step SplitterStep.segment_splits_exploded at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_4_segments_splits\n",
      "exploded_df\n",
      "flat_splits_df\n",
      "Calling default write for step SplitterStep.segment_splits_exploded at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_4_segments_splits\n",
      "Calling default write for step SplitterStep.segment_splits_exploded at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_4_segments_splits\n",
      "+----------+-------------------------------------------------------------------+------+\n",
      "|is_success|coalesce(element_at(split(error_message, :, -1), 1), error_message)|count |\n",
      "+----------+-------------------------------------------------------------------+------+\n",
      "|false     |Unexpected number of split points                                  |61    |\n",
      "|true      |                                                                   |164774|\n",
      "+----------+-------------------------------------------------------------------+------+\n",
      "\n",
      "+----------------+--------+\n",
      "|number_of_splits|count(1)|\n",
      "+----------------+--------+\n",
      "|               1|   36818|\n",
      "|               2|   17528|\n",
      "|               3|    8020|\n",
      "|               4|    3989|\n",
      "|               5|    2130|\n",
      "|               6|    1308|\n",
      "|               7|     844|\n",
      "|               8|     541|\n",
      "|               9|     387|\n",
      "|              10|     262|\n",
      "|              11|     194|\n",
      "|              12|     179|\n",
      "|              13|     112|\n",
      "|              14|      95|\n",
      "|              15|      83|\n",
      "|              16|      59|\n",
      "|              17|      43|\n",
      "|              18|      41|\n",
      "|              19|      39|\n",
      "|              20|      17|\n",
      "+----------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Calling default write for step SplitterStep.final_output at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_final\n",
      "Calling default write for step SplitterStep.final_output at file:///C:\\Users\\Adam\\Documents\\Python Scripts\\GIS/split_output//2024-11-13.0/filtered_final\n",
      "+---------+------+\n",
      "|     type| count|\n",
      "+---------+------+\n",
      "|connector|116846|\n",
      "|     NULL|    61|\n",
      "|  segment|164774|\n",
      "+---------+------+\n",
      "\n",
      "split segments metrics:\n",
      "164774\n",
      "+--------------------+-----------+-----------+----------+\n",
      "|                 key|      value|value_count|percentage|\n",
      "+--------------------+-----------+-----------+----------+\n",
      "|              length|B. 1cm-10cm|          1|       0.0|\n",
      "|              length| C. 10cm-1m|        875|      0.53|\n",
      "|              length| D. 1m-100m|     142946|     86.75|\n",
      "|              length|E. 100m-1km|      20642|     12.53|\n",
      "|              length|F. 1km-10km|        306|      0.19|\n",
      "|              length|   G. >10km|          4|       0.0|\n",
      "|original_self_int...|       true|         39|      0.02|\n",
      "|split_self_inters...|       true|          4|       0.0|\n",
      "+--------------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transportation_splitter\n",
    "import pyspark\n",
    "\n",
    "def retrieve_and_split(spark: SedonaContext,\n",
    "                       sc: pyspark.SparkContext,\n",
    "                       base_output_path: str,\n",
    "                       overture_release_version: str=\"2024-11-13.0\",\n",
    "                       overture_release_path: str=\"s3a://overturemaps-us-west-2/release\",\n",
    "                       wkt_filter: str=\"POLYGON ((-75.247 40.038, -75.247 39.870, -75.077 39.870, -75.077 40.038, -75.247 40.038))\") -> pyspark.sql.DataFrame:\n",
    "\n",
    "    # Some of this copied from the end of transportation_splitter.py\n",
    "    input_path = f\"{overture_release_path}/{overture_release_version}/theme=transportation\"\n",
    "    filter_target = \"global\" if not wkt_filter else \"filtered\"\n",
    "    output_path_prefix = f\"{base_output_path}/{overture_release_version}/{filter_target}\"\n",
    "\n",
    "    wrangler = transportation_splitter.SplitterDataWrangler(input_path=input_path, output_path_prefix=output_path_prefix)\n",
    "    result_df = transportation_splitter.split_transportation(spark, sc, wrangler, wkt_filter)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "spark = get_local_context()\n",
    "# Useful for fixing broken installs! (if you can access stdout)\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\") \n",
    "\n",
    "import os\n",
    "# This might take some time...\n",
    "philly_df = retrieve_and_split(spark, spark.sparkContext, f\"file:///{os.getenv(\"GIS_DIR\")}/split_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that took me over an hour (PySpark is slow to write GeoParquet files on my machine, apparently)! Everything's saved to the disk, but for convenience, I'll serialize the output in case something happens to my session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# If re-opening the notebook later after running everything, start from this cell\n",
    "if 'philly_df' in locals():\n",
    "    with open('philly_split_df.pickle', 'wb') as handle:\n",
    "        df = philly_df.toPandas()\n",
    "        pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('philly_split_df.pickle', 'rb') as handle:\n",
    "        df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Split Data\n",
    "\n",
    "Let's peek inside the output. I'm going to switch to an in-memory representation of our data (Pandas) for performance reasons (I do not have a cluster at my disposal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 281681 entries, 0 to 281680\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count   Dtype   \n",
      "---  ------                  --------------   -----   \n",
      " 0   id                      281620 non-null  object  \n",
      " 1   geometry                281620 non-null  geometry\n",
      " 2   bbox                    268291 non-null  object  \n",
      " 3   version                 268291 non-null  float64 \n",
      " 4   sources                 268291 non-null  object  \n",
      " 5   subtype                 164774 non-null  object  \n",
      " 6   class                   159709 non-null  object  \n",
      " 7   names                   67293 non-null   object  \n",
      " 8   connectors              164774 non-null  object  \n",
      " 9   routes                  5792 non-null    object  \n",
      " 10  subclass                31068 non-null   object  \n",
      " 11  subclass_rules          55219 non-null   object  \n",
      " 12  access_restrictions     51661 non-null   object  \n",
      " 13  level_rules             2341 non-null    object  \n",
      " 14  destinations            1087 non-null    object  \n",
      " 15  prohibited_transitions  992 non-null     object  \n",
      " 16  road_surface            45456 non-null   object  \n",
      " 17  road_flags              3882 non-null    object  \n",
      " 18  speed_limits            13356 non-null   object  \n",
      " 19  width_rules             715 non-null     object  \n",
      " 20  type                    281620 non-null  object  \n",
      " 21  start_lr                164774 non-null  float64 \n",
      " 22  end_lr                  164774 non-null  float64 \n",
      " 23  metrics                 164774 non-null  object  \n",
      "dtypes: float64(3), geometry(1), object(20)\n",
      "memory usage: 51.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "gdf = gpd.GeoDataFrame(df, crs=\"EPSG:4326\")\n",
    "print(gdf.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we've got ~250k edges with plenty of attributes. Let's make sure that each of these edges connects to just two nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connectors | count\n",
      "         2 | 164393\n",
      "         3 | 233\n",
      "         4 | 66\n",
      "         5 | 23\n",
      "         6 | 21\n",
      "         7 | 7\n",
      "         8 | 9\n",
      "         9 | 7\n",
      "        10 | 4\n",
      "        11 | 5\n",
      "        12 | 1\n",
      "        13 | 1\n",
      "        15 | 1\n",
      "        32 | 1\n",
      "        33 | 1\n",
      "        46 | 1\n"
     ]
    }
   ],
   "source": [
    "# Time to see if our split worked!\n",
    "print(\"connectors | count\")\n",
    "conn, count = np.unique([len(e) for e in gdf[gdf['type']=='segment'].connectors], return_counts=True)\n",
    "for cn, co in zip(conn, count):\n",
    "    print(f\"{cn:10} | {co}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm... *most* of them do, but there are a few hundred stragglers in there. Let's look at those and see if we can figure out what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c571a48594640cb86366c761c5fba0c",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Map(basemap_style=<CartoBasemap.Positron: 'https://basemaps.cartocdn.com/gl/positron-gl-style/style.json'>, cu…"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf = gpd.GeoDataFrame(df, crs=\"EPSG:4326\")\n",
    "gdf_segments = gdf[gdf['type']=='segment']\n",
    "gdf_connectors = gdf[gdf['type']=='connector']\n",
    "\n",
    "non_binary_segments = gdf_segments[gdf_segments.connectors.map(len) > 2]\n",
    "\n",
    "# lonboard wrapper to make for easy visualization of GeoDataFrames\n",
    "import lonboard\n",
    "from lonboard.basemap import CartoBasemap\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from matplotlib import colormaps, colors\n",
    "def display_with_lonboard(geo_data: gpd.GeoDataFrame,\n",
    "                          node_colors: np.ndarray[np.floating] | None=None,\n",
    "                          edge_colors: np.ndarray[np.floating] | None=None,\n",
    "                          mpl_colormap: str = 'viridis') -> lonboard.Map:\n",
    "    \n",
    "    # Generate geometry color lists, if needed\n",
    "    def gen_color_list(data):\n",
    "        normalized_attribs = colors.Normalize(np.min(data), np.max(data))(data)\n",
    "        cols = apply_continuous_cmap(normalized_attribs, colormaps[mpl_colormap])\n",
    "        return cols\n",
    "    edge_color_list = [219, 12, 4] # red\n",
    "    if edge_colors is not None:\n",
    "        edge_color_list = gen_color_list(edge_colors)\n",
    "    node_color_list = [100, 44, 191] # purple\n",
    "    if node_colors is not None:\n",
    "        node_color_list = gen_color_list(node_colors)\n",
    "    \n",
    "    # Convert columns with non-primitive dtypes to strings\n",
    "    # ... we need to do this, or lonboard throws an error\n",
    "    geo_data_parseable = gpd.GeoDataFrame()\n",
    "    for col, type_ in zip(geo_data.keys(), geo_data.dtypes):\n",
    "        if type_ == object:\n",
    "            geo_data_parseable[col] = geo_data[col].apply(lambda x : str(x))\n",
    "        else:\n",
    "            if col == 'geometry':\n",
    "                # Geopandas asked me to do this -- automatic geometry id'ing will be deprecated in the future\n",
    "                geo_data_parseable.set_geometry(geo_data[col], inplace=True)\n",
    "            else:\n",
    "                geo_data_parseable[col] = geo_data[col]\n",
    "\n",
    "    map_ = lonboard.viz(geo_data_parseable,\n",
    "                        path_kwargs={'get_color': edge_color_list},\n",
    "                        scatterplot_kwargs={'get_fill_color': node_color_list},\n",
    "                        map_kwargs={'basemap_style': CartoBasemap.Positron})\n",
    "    return map_\n",
    "\n",
    "display_with_lonboard(non_binary_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: you can try using `GeoDataFrame.explore()`, but [lonboard](https://developmentseed.org/lonboard/latest/) is more performant and won't crash your notebook when drawing >250k LineStrings.*\n",
    "\n",
    "Okay, it's clear that we're still retaining goemetry on the spatial filter border (literal 'edge cases'). This isn't a huge problem; we can just prune all references to connectors absent from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_id_set = set(gdf_connectors['id'])\n",
    "\n",
    "# GeoPandas assign() has some issues with SettingWithCopyWarning...\n",
    "# I'll make a copy, instead.\n",
    "gdf_edges = gpd.GeoDataFrame(gdf_segments)\n",
    "gdf_nodes = gpd.GeoDataFrame(gdf_connectors)\n",
    "gdf_edges['binary_connectors'] = gdf_segments.loc[:, 'connectors'].apply(\n",
    "    lambda conns : [i for i in conns if i['connector_id'] in connector_id_set]\n",
    ")\n",
    "\n",
    "# This actually *could* fail, but only if our segments 'leave' and 're-enter' the filter region.\n",
    "# Thankfully, that doesn't happen in the Philadelphia polygon!\n",
    "assert max([len(x) for x in gdf_edges['binary_connectors']])\n",
    "\n",
    "# For visual inspection, here's a hack to display each edge with a random color:\n",
    "# display_with_lonboard(gdf_edges, edge_colors=np.array([hash(s) for s in gdf_edges['id'].to_list()]), mpl_colormap='gist_rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Routing Graph\n",
    "\n",
    "We have an [adjacency list](https://en.wikipedia.org/wiki/Adjacency_list) in RAM — let's dump it into NetworkX.\n",
    "\n",
    "First, we're missing a curcial piece of information: path length (edge weight). For roads, this is a complicated problem dependent on many, many factors (traffic, road conditions, vehicle type, etc.). I don't want to clog up this notebook, so let's work with *footpaths*, instead.\n",
    "\n",
    "If you pan over the map above, you might notice that footpath data isn't complete: many streets only have segments *on the streets*, and sidewalks are not labeled. Consequently, we need to include ordinary roads in our footpath routing network, but ideally not when sidewalks are present. This is a not-so-easy problem, but we can employ a stopgap: we cost roads *a little bit* higher than footpaths. This might affect our routing a bit, but the alternative would be to try and algorithmically classify parallel footpaths / roads, and I don't have time to write that right now :)\n",
    "\n",
    "Second, we need to consider access restrictions: not all streets are two-way, and highway off-ramps are not always available in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting nodes...\n",
      "Inserting edges...\n"
     ]
    }
   ],
   "source": [
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import pyproj\n",
    "\n",
    "# Schema is defined here:\n",
    "# https://docs.overturemaps.org/guides/transportation/#subtypes\n",
    "# One thing to note: this schema doesn't have a type for subway lines (yet)!\n",
    "# However, subways are still *in* Overture's data, because Overture gets some of its data from OSM.\n",
    "\n",
    "# \"[subtype] [class]\" : [path preference (higher = more preferred)]\n",
    "PATH_PREFERENCE = {\n",
    "    'road footway' : 1.05,\n",
    "    'road pedestrian' : 1.05,\n",
    "    'road cycleway' : 1.0,\n",
    "    'road residential' : 1.0,\n",
    "    'road steps' : 1.0,\n",
    "    'road path' : 1.0,\n",
    "    'road living street' : 1.0,\n",
    "    'road primary' : 1.0,\n",
    "    'road secondary' : 1.0,\n",
    "    'road tertiary' : 1.0,\n",
    "    'road service' : 1.0,\n",
    "    'road trunk': 1.0\n",
    "}\n",
    "\n",
    "# We don't want Mercator distortion, so we'll have to use a better distance metric:\n",
    "geodetic_datum = pyproj.Geod(ellps=\"WGS84\")\n",
    "\n",
    "def add_to_network(G: nx.MultiDiGraph, row: pd.core.series.Series) -> None:\n",
    "    match row['type']:\n",
    "        case 'connector':\n",
    "            G.add_node(row['id'], id=row['id'], x=row['geometry'].x, y=row['geometry'].y)\n",
    "        case 'segment':\n",
    "            segment_subtype_and_class = str(row['subtype']) + ' ' + str(row['class'])\n",
    "            if segment_subtype_and_class in PATH_PREFERENCE.keys():\n",
    "                distance = geodetic_datum.geometry_length(row.geometry.coords)\n",
    "\n",
    "                LR_length = row.binary_connectors[1]['at'] - row.binary_connectors[0]['at']\n",
    "                distance *= LR_length # LR_length should always be 1.0, except on filter poly edges\n",
    "                \n",
    "                # Cost preferred paths lightly\n",
    "                preference = 1.0 / PATH_PREFERENCE[segment_subtype_and_class]\n",
    "                distance *= preference\n",
    "\n",
    "                n0 = row.binary_connectors[0]['connector_id']\n",
    "                n1 = row.binary_connectors[1]['connector_id']\n",
    "                \n",
    "                # OSMnx requires a directed graph, so we add each segment in both directions\n",
    "                # ...there are no one-way footpaths, right? right???\n",
    "                # Also, we include the edge LineString geometry for visualization purposes\n",
    "                G.add_edge(n0, n1,\n",
    "                           w=distance,\n",
    "                           segment_subtype=row['subtype'],\n",
    "                           segment_class=row['class'],\n",
    "                           geometry=row['geometry'],\n",
    "                           id=row['id']\n",
    "                           )\n",
    "\n",
    "        case _:\n",
    "            raise ValueError(\"Unknown row type in GeoDataFrame: \" + row['type'])\n",
    "\n",
    "def display_graph_with_lonboard(G_):\n",
    "    for_viz = ox.graph_to_gdfs(G_)[1] # edges\n",
    "    return display_with_lonboard(\n",
    "        for_viz,\n",
    "        edge_colors=np.clip(np.array(for_viz['w']), 0.0, 300.0) # dirty way to set color range\n",
    "    )\n",
    "\n",
    "print(\"Inserting nodes...\")\n",
    "G = nx.MultiGraph()\n",
    "G.graph['crs'] = \"EPSG:4326\"\n",
    "for index, row in gdf_nodes.iterrows():\n",
    "    add_to_network(G, row)\n",
    "\n",
    "print(\"Inserting edges...\")\n",
    "for index, row in gdf_edges.iterrows():\n",
    "    add_to_network(G, row)\n",
    "\n",
    "G = G.to_directed()\n",
    "G.remove_nodes_from(list(nx.isolates(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf17f87285794317b4070de27b34e845",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Map(basemap_style=<CartoBasemap.Positron: 'https://basemaps.cartocdn.com/gl/positron-gl-style/style.json'>, cu…"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_graph_with_lonboard(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a NetworkX graph now! This means we have access to [an incredible list](https://networkx.org/documentation/stable/reference/algorithms/index.html) of algorithms and statistics, as well as [OSMnx's utilities](https://osmnx.readthedocs.io/en/stable/user-reference.html). This includes exporting to OSM XML (`ox.io.save_graph_xml`), which could be handy for use with other routing engines (though we'd probably need to add a little more data, first).\n",
    "\n",
    "Let's try a few of these tools:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge count decreased by 15.1%.\n"
     ]
    }
   ],
   "source": [
    "# Details:\n",
    "# https://osmnx.readthedocs.io/en/stable/user-reference.html#osmnx.simplification.simplify_graph\n",
    "G_simp = ox.simplification.simplify_graph(G, edge_attr_aggs={'w' : sum})\n",
    "print(f\"Edge count decreased by {100.0 - len(G_simp.edges) / len(G.edges)*100:.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of Largest Connected Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes:       111744\n",
      "Largest component: 109634\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total nodes:       {len(G)}\")\n",
    "component_list = list(nx.weakly_connected_components(G))\n",
    "largest_component = list(component_list[np.argmax([len(c) for c in component_list])])\n",
    "print(f\"Largest component: {len(largest_component)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08f2aacb24616943043efbdc340e4e49 08f2a13482a46b4d047f759674730d81\n",
      "Path length: 15.6 km\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aae1a552377485bbfd5f48f095997c3",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Map(basemap_style=<CartoBasemap.Positron: 'https://basemaps.cartocdn.com/gl/positron-gl-style/style.json'>, cu…"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a couple of random nodes (in the same connected component, of course)\n",
    "# Realistically, you would want to use coordinates, then jump to a LR'd point on the nearest segment\n",
    "start = np.random.choice(largest_component)\n",
    "finish = np.random.choice(largest_component)\n",
    "\n",
    "print(start, finish)\n",
    "\n",
    "# ox.routing actually fails here (I guess the network is too big), so I use networkx!\n",
    "# I'm not worried about performance, lol; we're not making a proper routing engine :)\n",
    "path = nx.dijkstra_path(G, start, finish)\n",
    "\n",
    "# Extract adjacent pairs of nodes and put their geometry into a GeoDataFrame\n",
    "path_data = [G.get_edge_data(a, b)[0] for a, b in zip(path[:-1], path[1:])]\n",
    "\n",
    "print(f\"Path length: {0.001*sum([x['w'] for x in path_data]):.3} km\")\n",
    "display_with_lonboard(gpd.GeoDataFrame(path_data, crs=\"EPSG:4326\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The routing here is still a little wonky, as expected, but not terrible! We're still a long way from using Overture's data with something like [Valhalla](https://github.com/valhalla/valhalla), but exploratory projects like these give a good feel for the schema's kinks and features.\n",
    "\n",
    "An important issue we didn't address here is safety in pedestrian routing; check out [Walk Roll Map](https://walkrollmap.org/about) for a cool crowdsourcing project related to this issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
