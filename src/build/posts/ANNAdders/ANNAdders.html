<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="format-detection" content="telephone=no">

  <meta name="HandheldFriendly" content="true"/>
  <meta name="MobileOptimized" content="320"/>
  <meta name="viewport" content="initial-scale=1.0, width=device-width"/>

  <title>Adam Lastowka - Binary Universal Approximation & Neural Logic</title>
  <meta name="description" content="" />
  <script async defer src="./../../scripts/parallax.js" onload="var loaded=true;"></script>
  

  <link rel="stylesheet" id="lightmodehl" href="./../../scripts/highlight/styles/atom-one-light.min.css">
  <link rel="stylesheet" id="darkmodehl" href="./../../scripts/highlight/styles/hybrid.min.css" disabled="true">
  <script src="./../../scripts/highlight/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>

  <link rel="stylesheet" href="./../../post.css">
  <script defer src="./../../scripts/darkmode.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ["\\(", "\\)"]],
      displayMath: [['$$', '$$'], ["\\[", "\\]"]],
      processEscapes: true,
    },
    startup: {
      pageReady: () => {
        return MathJax.typesetPromise();
      }
    }
  };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  
  <link rel="icon" href="./../../favicon.ico" type="image/x-icon"/>
  <link rel="shortcut icon" href="./../../favicon.ico" type="image/x-icon"/>

</head>

<body>
  <div id ="bkg">

  <section id="not-background">
    <section id="sidebar">
      <div class="sticky">
        <div class="sb-big onbkg hvr-rotate"><a href="../../index.html">Home</a></div><br>
        <div class="sb-big onbkg hvr-rotate"><a href="../../about/index.html">About</a></div><br>
        <div class="sb-big onbkg hvr-rotate"><a href="../../qa/index.html">Q&A</a></div>
        <!--<br><br><div id="dm-toggle" class="button sb-big hvr-rotate">Lights</button>-->
      </div>
    </section>

    <section id="header">

      <div class="null">
        <div id="spikes" class="vector" alt=""></div>
      </div>

      <div id="sitename">
        <h5>
          <a href="./../../index.html">Adam<br>Lastowka</a>
        </h5>
      </div>
    </section>

    <div id="foreground">

      <section id="feed">
        <h1>Binary Universal Approximation & Neural Logic</h1>
        <div class="date">
          Published: 2024-09-09
        </div>
        <div class="tags">
          Tagged with:
          
            
              
                <a href="./../../topics/math/index.html">math</a>, 
              
                <a href="./../../topics/ml/index.html">ml</a>, 
              
                <a href="./../../topics/cs/index.html">cs</a>
              
            
          
        </div>
        <br>

        <h3>The Depth Necessary for Addition</h3>
<p>In 2023, <a href="https://cprimozic.net/">Casey Primozic</a> wrote <a href="https://cprimozic.net/blog/reverse-engineering-a-small-neural-network/">an awesome blog post</a> about training a small neural network to perform modular 8-bit addition. The network's solution was interesting — you can read about it in the post; it's not unlike what a similar network <a href="https://www.lesswrong.com/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking#Modular_Addition">reverse-engineered by Neil Nanda and Tom Lieberum in 2022</a> achieved. Here, I want to focus on a hypothesis Casey stated at the beginning of his post:</p>
<blockquote>
<blockquote>
<p><em>Additionally, I wasn't sure how the network would handle long chains of carries. When adding 11111111 + 00000001, for example, it wraps and produces an output of 00000000. In order for that to happen, the carry from the least-significant bit needs to propagate all the way through the adder to the most-significant bit. I thought that there was a good chance the network would need at least 8 layers in order to facilitate this kind of behavior.</em></p>
</blockquote>
</blockquote>
<p>Obviously, Casey then showed that this was false, but it's a very reasonable way of thinking. Look at a circuit diagram of <a href="https://en.wikipedia.org/wiki/Carry-lookahead_adder#Implementation_details">any adder</a>, and you'll probably see a sequential chain of gates flowing from lower to higher bit significance. For $ n $ bits, <a href="https://en.wikipedia.org/wiki/Kogge%E2%80%93Stone_adder">the best adder circuits</a> still need $ \log_2(n) $ layers. Additionally, the DAC/ADC-style solution Casey's network found might not scale well, and it would likely hit the floating-point precision floor as its bit count increased (this might be interesting to test). Maybe adders <em>do</em> need depth.</p>
<p>But wait, what about <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem#Arbitrary_width">universal approximation</a>?</p>
<p>Actually, circuit theory also has something to say here. Neural networks have what's called "unlimited <a href="https://en.wikipedia.org/wiki/Fan-in">fan-in</a>" — a neuron can have as many inputs as it likes. Couple this with a nonlinear activation function, and you'll find that, with the right parameters, individual neurons can perform AND and OR operations on an arbitrary number of incoming 'bits', as we'll see soon.</p>
<p>Because neurons have unlimited fan-in, feed-forward neural nets should be able to do addition with a constant number of layers, regardless of how many bits they're operating on: addition belongs to the $ AC^0 $ circuit complexity class, which has $ O(1) $ depth and polynomial size. Let's see how we can construct such a network explicitly.</p>
<h3>Neuron Logic</h3>
<p>First, we need to recognize that conventional neural networks can act as binary operators. A single ReLU-activated neuron can act as an AND gate:
<img alt="image" src="reluand.png" /></p>
<p>While it takes two ReLU neurons to make an OR:
<img alt="image" src="reluor.png" />
With a sigmoid activation, you can use a single neuron and saturate it by cranking up the weights.</p>
<p>Making a NOT gate is also easy (weight -1, bias 1). We could call it here and cite some circuit theory textbook, but let's explore a naive way to actually develop a $ O(1) $ (i.e. constant width) $ n $-bit adder.</p>
<h3>"Coding" an Adder</h3>
<p>Let's consider a 2-bit adder. We can explicitly write out the truth table for one of these:
<img alt="image" src="2bittruth.png" /></p>
<p>There are 16 entries in this truth table, representing each pair of 2-bit numbers that could be added together. The truth table for a 16-bit adder would have $ 2^{32} $, or 4 billion entries.</p>
<p>The easiest way to translate this into a constant-depth circuit? Encode each of these four billion rules into its logic. This means that our initial 16-bit circuit will have billions of transistors, but that's fine for now, we're just exploring.</p>
<p>First, we consider each digit of the output separately. Then, for each rule in the truth table with an output of '1', we make a circuit that activates <em>if and only if</em> each precondition in the table is met. Finally, we join the outputs of all these circuits with an unlimited fan-in OR gate. For our purposes, this is equivalent to writing the logic for each output digit in its Disjunctive Normal Form (DNF).</p>
<p>For example, for input binary numbers $ (a_2, a_1) $ and $ (b_2, b_1) $, the 10s place bit is given by:
$$ (a_1 \land b_1) \oplus (a_2 \oplus b_2) $$
Which, in DNF, expands to:
$$ (\lnot a_2 \land \lnot a_1 \land b_2) \lor(\lnot a_2 \land b_2 \land \lnot b_1) \lor \cdots \lor (a_2 \land a_1 \land b_2 \land b_1) $$
As a circuit, that expression looks like this:
<img alt="image" src="10splacecircuit.png" />
If you're doing this with neurons, you can make things a little more compact by folding the NOTs into the AND gate's weights.</p>
<p>This technique generalizes to <em>any</em> arbitrary truth table. However, it's a pretty disgusting way to make a circuit. Addition is a low-entropy operation; it has exploitable symmetries that suggest more efficient constant-depth representations. For $ n $ bits, this one has a width (AND gate count) in $ O(4^n) $. You can get that down to a 3-layer $ O(n^3) $ if you're <a href="https://www.csa.iisc.ac.in/~chandan/courses/arithmetic_circuits/notes/lec4.pdf">clever about it</a> (remember, addition is in $ AC^0 $, which means polynomial size). That's not too bad! For a 16-bit adder, $ O(n^3) $ puts us on the order of $ 10^4 $ gates, which is reasonable for a neural network.</p>
<p>Interestingly, the fact that we can construct this kind of a network at all <strong>demonstrates a binary universal approximation theorem</strong>. All data on computers is stored in binary, and this shows that a shallow neural network can transform that data in any way you want.</p>
<h3>In the Wild</h3>
<p>But, like all the other universal approximation theorems, it's not very helpful when you're trying to train something. If a network memorizes its inputs without 'understanding' them, we call it "overfitting". Sometimes this is okay: networks can still <a href="https://www.beren.io/2022-01-11-Grokking-Grokking/">grok</a> data after overfitting it, especially in sufficiently-overparameterized networks. Maybe that initial gradient rush to the solution manifold sometimes takes us to a network similar to the DNF nets described above? Maybe DNF is a good starting point?</p>
<p>No, and probably not.</p>
<p>First, neural nets rely heavily on dense layers, where neurons are fully connected to each other. The type of network we've constructed is extremely sparse, and gradient descent is unlikely to accidentally stumble across anything like it unless it's being heavily regularized. I've been trying to coax a network into this sort of representation for a while, but without fixing the topology via weight masking, it's incredibly challenging.</p>
<p>Second, DNF can't really <em>be</em> abstract; it lays everything out on the table, and doesn't permit any clever representation tricks. Densely-connecting a DNF network <em>might</em> be a good starting point for understanding grokking, though (or slowly 'activating' additional weights), but probably not on anything of significant size.</p>
<h3>Takeaways</h3>
<ul>
<li>Any boolean function can be represented as a constant-depth neural network by writing each output bit as an expression of the input bits in disjunctive normal form, then creating a logical (neural) circuit from that expression.</li>
<li>However, these networks are sparsely-connected, and non-polynomial in width, and probably incapable of generalizing.</li>
</ul>

        
          <a href="./../../posts\SymbolicCircuits\SymbolicCircuits.html" class="button2" style="min-width:47%;">Previous Post:<br></a>
        

        
          <a href="./../../posts\draft\UnorderedList.html" class="button2" style="min-width:47%;">Next Post:<br></a>
        
        
      </section>
      
      <script>
        if (window.MathJax) {
          MathJax.typesetPromise().catch(err => console.log(err));
        }
      </script>

      <div id="footer">

      </div>
    </div>
  </section>
</div>
</body>

</html>